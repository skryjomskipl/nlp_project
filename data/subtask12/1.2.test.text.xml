<?xml version="1.0" encoding="UTF-8"?>
<doc>

<text id="I08-2123"><title>
A <entity id="I08-2123.1">Co-occurrence</entity> <entity id="I08-2123.2">Graph-based Approach</entity> for Personal <entity id="I08-2123.3">Name</entity> <entity id="I08-2123.4">Alias</entity> <entity id="I08-2123.5">Extraction</entity> from Anchor <entity id="I08-2123.6">Texts</entity></title><abstract>
A person may have multiple <entity id="I08-2123.7">name</entity> <entity id="I08-2123.8">aliases</entity> on the Web. <entity id="I08-2123.9">Identifying</entity> <entity id="I08-2123.10">aliases</entity> of a <entity id="I08-2123.11">name</entity> is important for various <entity id="I08-2123.12">tasks</entity> such as <entity id="I08-2123.13">information retrieval</entity>, <entity id="I08-2123.14">sentiment analysis</entity> and <entity id="I08-2123.15">name</entity> <entity id="I08-2123.16">disambiguation</entity>. We introduce the <entity id="I08-2123.17">notion</entity> of a <entity id="I08-2123.18">word</entity> <entity id="I08-2123.19">co-occurrence</entity> graph to represent the mutual <entity id="I08-2123.20">relations</entity> between <entity id="I08-2123.21">words</entity> that appear in <entity id="I08-2123.22">anchor texts</entity>. <entity id="I08-2123.23">Words</entity> in <entity id="I08-2123.24">anchor texts</entity> are represented as <entity id="I08-2123.25">nodes</entity> in the <entity id="I08-2123.26">co-occurrence</entity> graph and an <entity id="I08-2123.27">edge</entity> is <entity id="I08-2123.28">formed</entity> between <entity id="I08-2123.29">nodes</entity> which <entity id="I08-2123.30">link</entity> to the same url. For a given personal <entity id="I08-2123.31">name</entity>, its <entity id="I08-2123.32">neighboring</entity> <entity id="I08-2123.33">nodes</entity> in the graph are considered as <entity id="I08-2123.34">candidates</entity> of its <entity id="I08-2123.35">aliases</entity>. We formalize <entity id="I08-2123.36">alias</entity> <entity id="I08-2123.37">identification</entity> as a <entity id="I08-2123.38">problem</entity> of ranking <entity id="I08-2123.39">nodes</entity> in this graph with <entity id="I08-2123.40">respect</entity> to a given <entity id="I08-2123.41">name</entity>. We integrate various ranking scores through <entity id="I08-2123.42">support vector machines</entity> to <entity id="I08-2123.43">leverage</entity> a <entity id="I08-2123.44">robust</entity> ranking <entity id="I08-2123.45">function</entity> and use it to <entity id="I08-2123.46">extract</entity> <entity id="I08-2123.47">aliases</entity> for a given <entity id="I08-2123.48">name</entity>. <entity id="I08-2123.49">Experimental</entity> <entity id="I08-2123.50">results</entity> on a dataset of <entity id="I08-2123.51">Japanese</entity> celebrities show that the <entity id="I08-2123.52">proposed</entity> <entity id="I08-2123.53">method</entity> outperforms all baselines, <entity id="I08-2123.54">displaying</entity> a MRR score of 0.562.
</abstract>


</text>

<text id="W06-2801"><title><entity id="W06-2801.1">Text</entity> Linkage In The Wiki Medium - A Comparative <entity id="W06-2801.2">Study</entity></title><abstract>
We analyze four different <entity id="W06-2801.3">types</entity> of <entity id="W06-2801.4">document</entity> <entity id="W06-2801.5">networks</entity> with <entity id="W06-2801.6">respect</entity> to their small world <entity id="W06-2801.7">characteristics</entity>. These <entity id="W06-2801.8">characteristics</entity> allow distinguishing <entity id="W06-2801.9">wiki-based systems</entity> from <entity id="W06-2801.10">citation</entity> and more traditional <entity id="W06-2801.11">text-based</entity> <entity id="W06-2801.12">networks</entity> augmented by hyperlinks. The <entity id="W06-2801.13">study</entity> <entity id="W06-2801.14">provides</entity> <entity id="W06-2801.15">evidence</entity> that a more appropriate <entity id="W06-2801.16">network</entity> <entity id="W06-2801.17">model</entity> is needed which better reflects the specifics of wiki <entity id="W06-2801.18">systems</entity>. It puts emphasize on their topological <entity id="W06-2801.19">differences</entity> as a <entity id="W06-2801.20">result</entity> of <entity id="W06-2801.21">wiki-related</entity> <entity id="W06-2801.22">linking</entity> compared to other <entity id="W06-2801.23">text-based</entity> <entity id="W06-2801.24">networks</entity>.
</abstract>


</text>

<text id="W06-2802"><title>
Errors In Wikis
</title><abstract>
This <entity id="W06-2802.1">discussion</entity> <entity id="W06-2802.2">document</entity> <entity id="W06-2802.3">concerns</entity> the <entity id="W06-2802.4">challenges</entity> to <entity id="W06-2802.5">assessments</entity> of <entity id="W06-2802.6">reliability</entity> posed by wikis and the potential for <entity id="W06-2802.7">language processing</entity> <entity id="W06-2802.8">techniques</entity> for aiding readers to decide whether to trust particular <entity id="W06-2802.9">text</entity>.
</abstract>


</text>

<text id="W06-3115"><title>
NTT <entity id="W06-3115.1">System</entity> <entity id="W06-3115.2">Description</entity> For The WMT2006 <entity id="W06-3115.3">Shared Task</entity></title><abstract>
"We present two <entity id="W06-3115.4">translation systems</entity> <entity id="W06-3115.5">experimented</entity> for the <entity id="W06-3115.6">shared-task</entity> of ""<entity id="W06-3115.7">Workshop</entity> on <entity id="W06-3115.8">Statistical Machine Translation</entity>,"" a <entity id="W06-3115.9">phrase-based model</entity> and a hierarchical <entity id="W06-3115.10">phrase-based model</entity>. The former uses a phrasal <entity id="W06-3115.11">unit</entity> for <entity id="W06-3115.12">translation</entity>, whereas the latter is conceptualized as a synchronous-CFG in which <entity id="W06-3115.13">phrases</entity> are hierarchically combined using non-terminals. <entity id="W06-3115.14">Experiments</entity> showed that the hierarchical <entity id="W06-3115.15">phrase-based model</entity> <entity id="W06-3115.16">performed</entity> very comparable to the <entity id="W06-3115.17">phrase-based model</entity>. We also <entity id="W06-3115.18">report</entity> a <entity id="W06-3115.19">phrase</entity>/<entity id="W06-3115.20">rule</entity> <entity id="W06-3115.21">extraction</entity> <entity id="W06-3115.22">technique</entity> differentiating tokenization of <entity id="W06-3115.23">corpora</entity>. "
</abstract>


</text>

<text id="W06-1710"><title>
Web <entity id="W06-1710.1">Corpus</entity> Mining By <entity id="W06-1710.2">Instance</entity> Of Wikipedia
</title><abstract>
In this <entity id="W06-1710.3">paper</entity> we present an <entity id="W06-1710.4">approach</entity> to <entity id="W06-1710.5">structure</entity> learning in the <entity id="W06-1710.6">area</entity> of <entity id="W06-1710.7">web documents</entity>. This is done in <entity id="W06-1710.8">order</entity> to <entity id="W06-1710.9">approach</entity> the <entity id="W06-1710.10">goal</entity> of webgenre <entity id="W06-1710.11">tagging</entity> in the <entity id="W06-1710.12">area</entity> of web <entity id="W06-1710.13">corpus</entity> <entity id="W06-1710.14">linguistics</entity>. A central <entity id="W06-1710.15">outcome</entity> of the <entity id="W06-1710.16">paper</entity> is that purely <entity id="W06-1710.17">structure</entity> oriented <entity id="W06-1710.18">approaches</entity> to <entity id="W06-1710.19">web document</entity> <entity id="W06-1710.20">classification</entity> <entity id="W06-1710.21">provide</entity> an <entity id="W06-1710.22">information gain</entity> which may be utilized in combined <entity id="W06-1710.23">approaches</entity> of web <entity id="W06-1710.24">content</entity> and <entity id="W06-1710.25">structure</entity> <entity id="W06-1710.26">analysis</entity>.
</abstract>


</text>

<text id="W06-1906"><title>
BRUJA: <entity id="W06-1906.1">Question</entity> <entity id="W06-1906.2">Classification</entity> For Spanish Using <entity id="W06-1906.3">Machine</entity> Translation and An <entity id="W06-1906.4">English</entity> <entity id="W06-1906.5">Classifier</entity></title>
<abstract><entity id="W06-1906.6">Question</entity> <entity id="W06-1906.7">Classification</entity> is an important <entity id="W06-1906.8">task</entity> in <entity id="W06-1906.9">Question Answering</entity> <entity id="W06-1906.10">Systems</entity>. This <entity id="W06-1906.11">paper</entity> presents a Spanish <entity id="W06-1906.12">Question</entity> <entity id="W06-1906.13">Classifier</entity> <entity id="W06-1906.14">based</entity> on <entity id="W06-1906.15">machine learning</entity>, <entity id="W06-1906.16">automatic</entity> online <entity id="W06-1906.17">translators</entity> and different <entity id="W06-1906.18">language</entity> <entity id="W06-1906.19">features</entity>. Our <entity id="W06-1906.20">system</entity> works with <entity id="W06-1906.21">English</entity> <entity id="W06-1906.22">collections</entity> and bilingual <entity id="W06-1906.23">questions</entity> (<entity id="W06-1906.24">English</entity>/Spanish). We have <entity id="W06-1906.25">tested</entity> two Spanish-<entity id="W06-1906.26">English</entity> online <entity id="W06-1906.27">translators</entity> to identify the lost of <entity id="W06-1906.28">precision</entity>. We have made <entity id="W06-1906.29">experiments</entity> using <entity id="W06-1906.30">lexical</entity>, <entity id="W06-1906.31">syntactic</entity> and <entity id="W06-1906.32">semantic features</entity> to <entity id="W06-1906.33">test</entity> which ones made a better <entity id="W06-1906.34">performance</entity>. The obtained <entity id="W06-1906.35">results</entity> show that our <entity id="W06-1906.36">system</entity> makes good <entity id="W06-1906.37">classifications</entity>, over a 80% in <entity id="W06-1906.38">terms</entity> of <entity id="W06-1906.39">accuracy</entity> using the original <entity id="W06-1906.40">English</entity> <entity id="W06-1906.41">questions</entity> and over a 65% using Spanish <entity id="W06-1906.42">questions</entity> and <entity id="W06-1906.43">machine translation systems</entity>. Our <entity id="W06-1906.44">conclusion</entity> about the <entity id="W06-1906.45">features</entity> is that a <entity id="W06-1906.46">lexical</entity>, <entity id="W06-1906.47">syntactic</entity> and <entity id="W06-1906.48">semantic features</entity> <entity id="W06-1906.49">combination</entity> obtains the best <entity id="W06-1906.50">result</entity>.
</abstract>


</text>

<text id="I08-4033"><title>
Achilles: NiCT/ATR <entity id="I08-4033.1">Chinese</entity> Morphological <entity id="I08-4033.2">Analyzer</entity> for the Fourth Sighan Bakeoff
</title><abstract>
We created a new <entity id="I08-4033.3">Chinese</entity> morphological <entity id="I08-4033.4">analyzer</entity>, <PERSON>Achilles</PERSON> , by integrating <entity id="I08-4033.5">rule-based</entity>, <entity id="I08-4033.6">dictionary-based</entity>, and <entity id="I08-4033.7">statistical</entity> <entity id="I08-4033.8">machine</entity> learning <entity id="I08-4033.9">method</entity>, <entity id="I08-4033.10">conditional random fields</entity> (CRF). The <entity id="I08-4033.11">rule-based method</entity> is used to recognize regular <entity id="I08-4033.12">expressions</entity>: <entity id="I08-4033.13">numbers</entity>, <entity id="I08-4033.14">time</entity> and alphabets. The <entity id="I08-4033.15">dictionary-based method</entity> is used to find <entity id="I08-4033.16">in-vocabulary</entity> (IV) <entity id="I08-4033.17">words</entity> while <entity id="I08-4033.18">out-of-vocabulary</entity> (OOV) <entity id="I08-4033.19">words</entity> are detected by the CRFs. At last, <entity id="I08-4033.20">confidence</entity> measure <entity id="I08-4033.21">based</entity> <entity id="I08-4033.22">approach</entity> is used to weigh all the <entity id="I08-4033.23">results</entity> and <entity id="I08-4033.24">output</entity> the best ones. <PERSON>Achilles</PERSON> was used and <entity id="I08-4033.25">evaluated</entity> in the bakeoff. We participated the closed tracks of <entity id="I08-4033.26">word segmentation</entity> and <entity id="I08-4033.27">part-of-speech</entity> <entity id="I08-4033.28">tagging</entity> for all the <entity id="I08-4033.29">provided</entity> <entity id="I08-4033.30">corpus</entity>. In <entity id="I08-4033.31">spite</entity> of an unexpected file encoding <entity id="I08-4033.32">errors</entity>, the <entity id="I08-4033.33">system</entity> exhibited a top <entity id="I08-4033.34">level</entity> <entity id="I08-4033.35">performance</entity>. A higher <entity id="I08-4033.36">word segmentation</entity> <entity id="I08-4033.37">accuracy</entity> for the <entity id="I08-4033.38">corpus</entity> ckip and ncc were achieved. We are <entity id="I08-4033.39">ranked</entity> at the fifth and eighth position out of all 19 and 26 <entity id="I08-4033.40">submissions</entity> respectively for the two <entity id="I08-4033.41">corpus</entity>. Achilles uses a <entity id="I08-4033.42">feature</entity> combined <entity id="I08-4033.43">approach</entity> for <entity id="I08-4033.44">part-of-speech</entity> <entity id="I08-4033.45">tagging</entity>. Our <entity id="I08-4033.46">post-evaluation results</entity> prove the <entity id="I08-4033.47">effectiveness</entity> of this <entity id="I08-4033.48">approach</entity> for <entity id="I08-4033.49">POS tagging</entity>.
</abstract>


</text>

<text id="P07-1078"><title>
Self-<entity id="P07-1078.1">Training</entity> for <entity id="P07-1078.2">Enhancement</entity> and <entity id="P07-1078.3">Domain Adaptation</entity> of <entity id="P07-1078.4">Statistical</entity> Parsers <entity id="P07-1078.5">Trained</entity> on Small Datasets
</title><abstract>
Creating large <entity id="P07-1078.6">amounts</entity> of annotated <entity id="P07-1078.7">data</entity> to <entity id="P07-1078.8">train</entity> <entity id="P07-1078.9">statistical</entity> PCFG <entity id="P07-1078.10">parsers</entity> is expensive, and the <entity id="P07-1078.11">performance</entity> of such <entity id="P07-1078.12">parsers</entity> declines when <entity id="P07-1078.13">training</entity> and <entity id="P07-1078.14">test</entity><entity id="P07-1078.15">data</entity> are taken from different <entity id="P07-1078.16">domains</entity>. In this <entity id="P07-1078.17">paper</entity> we use <entity id="P07-1078.18">self-training</entity> in <entity id="P07-1078.19">order</entity> to <entity id="P07-1078.20">improve</entity> the <entity id="P07-1078.21">quality</entity> of a <entity id="P07-1078.22">parser</entity> and to <entity id="P07-1078.23">adapt</entity> it to a different <entity id="P07-1078.24">domain</entity>, using only small <entity id="P07-1078.25">amounts</entity> of manually annotated <entity id="P07-1078.26">seed</entity><entity id="P07-1078.27">data</entity>. We <entity id="P07-1078.28">report</entity> significant <entity id="P07-1078.29">improvement</entity> both when the <entity id="P07-1078.30">seed</entity> and <entity id="P07-1078.31">test</entity><entity id="P07-1078.32">data</entity> are in the same <entity id="P07-1078.33">domain</entity> and in the <entity id="P07-1078.34">out-of-domain adaptation</entity> <entity id="P07-1078.35">scenario</entity>. In particular, we achieve 50% <entity id="P07-1078.36">reduction</entity> in annotation <entity id="P07-1078.37">cost</entity> for the <entity id="P07-1078.38">in-domain</entity> <entity id="P07-1078.39">case</entity>, <entity id="P07-1078.40">yielding</entity> an <entity id="P07-1078.41">improvement</entity> of 66% over previous work, and a 20-33% <entity id="P07-1078.42">reduction</entity> for the <entity id="P07-1078.43">domain adaptation</entity> <entity id="P07-1078.44">case</entity>. This is the first <entity id="P07-1078.45">time</entity> that <entity id="P07-1078.46">self-training</entity> with small labeled datasets is <entity id="P07-1078.47">applied</entity> successfully to these <entity id="P07-1078.48">tasks</entity>. We were also able to formulate a <entity id="P07-1078.49">characterization</entity> of when <entity id="P07-1078.50">self-training</entity> is valuable.
</abstract>


</text>

<text id="W05-1104"><title>
Designing an Extensible API for Integrating <entity id="W05-1104.1">Language Modeling</entity> and <entity id="W05-1104.2">Realization</entity></title><abstract>
"We present an extensible API for integrating <entity id="W05-1104.3">language modeling</entity> and <entity id="W05-1104.4">realization</entity>, describing its <entity id="W05-1104.5">design</entity> and efficient <entity id="W05-1104.6">implementation</entity> in the OpenCCG <entity id="W05-1104.7">surface</entity> realizer. With OpenCCG, <entity id="W05-1104.8">language models</entity> may be used to select <entity id="W05-1104.9">realizations</entity> with preferred <entity id="W05-1104.10">word</entity> <entity id="W05-1104.11">orders</entity>, promote <entity id="W05-1104.12">alignment</entity> with a conversational partner, avoid repetitive <entity id="W05-1104.13">language</entity> use, and <entity id="W05-1104.14">increase</entity> the <entity id="W05-1104.15">speed</entity> of the best-first anytime <entity id="W05-1104.16">search</entity>. The API enables a <entity id="W05-1104.17">variety</entity> of <entity id="W05-1104.18">n-gram models</entity> to be easily combined and used in <entity id="W05-1104.19">conjunction</entity> with appropriate <entity id="W05-1104.20">edge</entity> pruning <entity id="W05-1104.21">strategies</entity>. The <entity id="W05-1104.22">n-gram models</entity> may be of any <entity id="W05-1104.23">order</entity>, operate in reverse (""right-to-left""), and selectively replace certain <entity id="W05-1104.24">words</entity> with their <entity id="W05-1104.25">semantic classes</entity>. Factored <entity id="W05-1104.26">language models</entity> with <entity id="W05-1104.27">generalized</entity> backoff may also be employed, over <entity id="W05-1104.28">words</entity> represented as bundles of <entity id="W05-1104.29">factors</entity> such as <entity id="W05-1104.30">form</entity>, <entity id="W05-1104.31">pitch</entity> accent, <entity id="W05-1104.32">stem</entity>, <entity id="W05-1104.33">part of speech</entity>, supertag, and <entity id="W05-1104.34">semantic class</entity>. "
</abstract>


</text>

<text id="L08-1203">
<title>
KnoFusius: a New <entity id="L08-1203.1">Knowledge</entity> <entity id="L08-1203.2">Fusion</entity> <entity id="L08-1203.3">System</entity> for <entity id="L08-1203.4">Interpretation</entity> of <entity id="L08-1203.5">Gene</entity> <entity id="L08-1203.6">Expression</entity> <entity id="L08-1203.7">Data</entity></title>
<abstract>
This <entity id="L08-1203.8">paper</entity> introduces a new <entity id="L08-1203.9">architecture</entity> that aims at combining molecular <entity id="L08-1203.10">biology</entity> <entity id="L08-1203.11">data</entity> with <entity id="L08-1203.12">information</entity> automatically <entity id="L08-1203.13">extracted</entity> from relevant <entity id="L08-1203.14">scientific literature</entity> ( using <entity id="L08-1203.15">text</entity> mining <entity id="L08-1203.16">techniques</entity> on PubMed <entity id="L08-1203.17">abstracts</entity> and fulltext <entity id="L08-1203.18">papers</entity> ) to <entity id="L08-1203.19">help</entity> biomedical <entity id="L08-1203.20">experts</entity> to interpret <entity id="L08-1203.21">experimental</entity> <entity id="L08-1203.22">results</entity> in <entity id="L08-1203.23">hand</entity> . The infrastructural <entity id="L08-1203.24">level</entity> bears on <entity id="L08-1203.25">semantic-web</entity> <entity id="L08-1203.26">technologies</entity> and <entity id="L08-1203.27">standards</entity> that facilitate the actual <entity id="L08-1203.28">fusion</entity> of the <entity id="L08-1203.29">multi-source</entity> <entity id="L08-1203.30">knowledge</entity> .
</abstract>

</text>
</doc>