<?xml version="1.0" encoding="UTF-8" ?>
<doc>


<text id="H01-1001"> <title>Activity detection for information access to oral communication</title> <abstract> Test <entity id="H01-1001.1">Oral communication</entity> is ubiquitous and carries important information yet it is also time consuming to document. Given the development of <entity id="H01-1001.2">storage media and networks</entity> one could just record and store a <entity id="H01-1001.3">conversation</entity> for documentation. The question is, however, how an interesting information piece would be found in a <entity id="H01-1001.4">large database</entity> . Traditional <entity id="H01-1001.5">information retrieval techniques</entity> use a <entity id="H01-1001.6">histogram</entity> of <entity id="H01-1001.7">keywords</entity> as the <entity id="H01-1001.8">document representation</entity> but <entity id="H01-1001.9">oral communication</entity> may offer additional <entity id="H01-1001.10">indices</entity> such as the time and place of the rejoinder and the attendance. An alternative <entity id="H01-1001.11">index</entity> could be the activity such as discussing, planning, informing, story-telling, etc. This paper addresses the problem of the <entity id="H01-1001.12">automatic detection</entity> of those activities in meeting situation and everyday rejoinders. Several extensions of this basic idea are being discussed and/or evaluated: Similar to activities one can define subsets of larger <entity id="H01-1001.13">database</entity> and detect those automatically which is shown on a large <entity id="H01-1001.14">database</entity> of <entity id="H01-1001.15">TV shows</entity> . <entity id="H01-1001.16">Emotions</entity> and other <entity id="H01-1001.17">indices</entity> such as the <entity id="H01-1001.18">dominance distribution of speakers</entity> might be available on the <entity id="H01-1001.19">surface</entity> and could be used directly. Despite the small size of the <entity id="H01-1001.20">databases</entity> used some results about the effectiveness of these <entity id="H01-1001.21">indices</entity> can be obtained. </abstract>


</text>

<text id="H01-1017"> <title>Dialogue Interaction with the DARPA Communicator Infrastructure: The Development of Useful Software</title> <abstract> To support engaging human users in robust, <entity id="H01-1017.1">mixed-initiative speech dialogue interactions</entity> which reach beyond current capabilities in <entity id="H01-1017.2">dialogue systems</entity> , the <entity id="H01-1017.3">DARPA Communicator program</entity> [1] is funding the development of a <entity id="H01-1017.4">distributed message-passing infrastructure</entity> for <entity id="H01-1017.5">dialogue systems</entity> which all <entity id="H01-1017.6">Communicator</entity> participants are using. In this presentation, we describe the features of and <entity id="H01-1017.7">requirements</entity> for a genuinely useful <entity id="H01-1017.8">software infrastructure</entity> for this purpose. </abstract>


</text>

<text id="H01-1041"> <title>Interlingua-Based Broad-Coverage Korean-to-English Translation in CCLING</title> <abstract> At MIT Lincoln Laboratory, we have been developing a <entity id="H01-1041.1">Korean-to-English machine translation system</entity> <entity id="H01-1041.2">CCLINC (Common Coalition Language System at Lincoln Laboratory)</entity> . The <entity id="H01-1041.3">CCLINC Korean-to-English translation system</entity> consists of two <entity id="H01-1041.4">core modules</entity> , <entity id="H01-1041.5">language understanding and generation modules</entity> mediated by a <entity id="H01-1041.6">language neutral meaning representation</entity> called a <entity id="H01-1041.7">semantic frame</entity> . The key features of the system include: (i) Robust efficient <entity id="H01-1041.8">parsing</entity> of <entity id="H01-1041.9">Korean</entity> (a <entity id="H01-1041.10">verb final language</entity> with <entity id="H01-1041.11">overt case markers</entity> , relatively <entity id="H01-1041.12">free word order</entity> , and frequent omissions of <entity id="H01-1041.13">arguments</entity> ). (ii) High quality <entity id="H01-1041.14">translation</entity> via <entity id="H01-1041.15">word sense disambiguation</entity> and accurate <entity id="H01-1041.16">word order generation</entity> of the <entity id="H01-1041.17">target language</entity> . (iii) <entity id="H01-1041.18">Rapid system development</entity> and porting to new <entity id="H01-1041.19">domains</entity> via <entity id="H01-1041.20">knowledge-based automated acquisition of grammars</entity> . Having been trained on <entity id="H01-1041.21">Korean newspaper articles</entity> on missiles and chemical biological warfare, the system produces the <entity id="H01-1041.22">translation output</entity> sufficient for content understanding of the <entity id="H01-1041.23">original document</entity> . </abstract>


</text>

<text id="H01-1042"> <title>Is That Your Final Answer?</title> <abstract> The purpose of this research is to test the efficacy of applying <entity id="H01-1042.1">automated evaluation techniques</entity> , originally devised for the evaluation of <entity id="H01-1042.2">human language learners</entity> , to the <entity id="H01-1042.3">output</entity> of <entity id="H01-1042.4">machine translation (MT) systems</entity> . We believe that these <entity id="H01-1042.5">evaluation techniques</entity> will provide information about both the <entity id="H01-1042.6">human language learning process</entity> , the <entity id="H01-1042.7">translation process</entity> and the <entity id="H01-1042.8">development</entity> of <entity id="H01-1042.9">machine translation systems</entity> . This, the first experiment in a series of experiments, looks at the <entity id="H01-1042.10">intelligibility</entity> of <entity id="H01-1042.11">MT output</entity> . A <entity id="H01-1042.12">language learning experiment</entity> showed that <entity id="H01-1042.13">assessors</entity> can differentiate <entity id="H01-1042.14">native from non-native language essays</entity> in less than 100 <entity id="H01-1042.15">words</entity> . Even more illuminating was the factors on which the <entity id="H01-1042.16">assessors</entity> made their decisions. We tested this to see if similar criteria could be elicited from duplicating the experiment using <entity id="H01-1042.17">machine translation output</entity> . Subjects were given a set of up to six extracts of <entity id="H01-1042.18">translated newswire text</entity> . Some of the extracts were <entity id="H01-1042.19">expert human translations</entity> , others were <entity id="H01-1042.20">machine translation outputs</entity> . The subjects were given three minutes per extract to determine whether they believed the sample output to be an <entity id="H01-1042.21">expert human translation</entity> or a <entity id="H01-1042.22">machine translation</entity> . Additionally, they were asked to mark the <entity id="H01-1042.23">word</entity> at which they made this decision. The results of this experiment, along with a preliminary analysis of the factors involved in the decision making process will be presented here. </abstract>


</text>

<text id="H01-1049"> <title>Listen-Communicate-Show (LCS): Spoken Language Command of Agent-based Remote Information Access</title> <abstract> <entity id="H01-1049.1">Listen-Communicate-Show (LCS)</entity> is a new paradigm for <entity id="H01-1049.2">human interaction with data sources</entity> . We integrate a <entity id="H01-1049.3">spoken language understanding system</entity> with <entity id="H01-1049.4">intelligent mobile agents</entity> that mediate between <entity id="H01-1049.5">users</entity> and <entity id="H01-1049.6">information sources</entity> . We have built and will demonstrate an application of this approach called <entity id="H01-1049.7">LCS-Marine</entity> . Using <entity id="H01-1049.8">LCS-Marine</entity> , tactical personnel can converse with their logistics system to place a supply or information request. The request is passed to a <entity id="H01-1049.9">mobile, intelligent agent</entity> for execution at the appropriate <entity id="H01-1049.10">database</entity> . <entity id="H01-1049.11">Requestors</entity> can also instruct the system to notify them when the status of a <entity id="H01-1049.12">request</entity> changes or when a <entity id="H01-1049.13">request</entity> is complete. We have demonstrated this capability in several field exercises with the Marines and are currently developing applications of this technology in <entity id="H01-1049.14">new domains</entity> . </abstract>


</text>

<text id="H01-1058"> <title>On Combining Language Models : Oracle Approach</title> <abstract> In this paper, we address the problem of combining several <entity id="H01-1058.1">language models (LMs)</entity> . We find that simple <entity id="H01-1058.2">interpolation methods</entity> , like <entity id="H01-1058.3">log-linear and linear interpolation</entity> , improve the <entity id="H01-1058.4">performance</entity> but fall short of the <entity id="H01-1058.5">performance</entity> of an <entity id="H01-1058.6">oracle</entity> . The <entity id="H01-1058.7">oracle</entity> knows the <entity id="H01-1058.8">reference word string</entity> and selects the <entity id="H01-1058.9">word string</entity> with the best <entity id="H01-1058.10">performance</entity> (typically, <entity id="H01-1058.11">word or semantic error rate</entity> ) from a list of <entity id="H01-1058.12">word strings</entity> , where each <entity id="H01-1058.13">word string</entity> has been obtained by using a different <entity id="H01-1058.14">LM</entity> . Actually, the <entity id="H01-1058.15">oracle</entity> acts like a <entity id="H01-1058.16">dynamic combiner</entity> with <entity id="H01-1058.17">hard decisions</entity> using the <entity id="H01-1058.18">reference</entity> . We provide experimental results that clearly show the need for a <entity id="H01-1058.19">dynamic language model combination</entity> to improve the <entity id="H01-1058.20">performance</entity> further . We suggest a method that mimics the behavior of the <entity id="H01-1058.21">oracle</entity> using a <entity id="H01-1058.22">neural network</entity> or a <entity id="H01-1058.23">decision tree</entity> . The method amounts to tagging <entity id="H01-1058.24">LMs</entity> with <entity id="H01-1058.25">confidence measures</entity> and picking the best <entity id="H01-1058.26">hypothesis</entity> corresponding to the <entity id="H01-1058.27">LM</entity> with the best <entity id="H01-1058.28">confidence</entity> . </abstract>


</text>

<text id="H01-1070"> <title>Towards an Intelligent Multilingual Keyboard System</title> <abstract> This paper proposes a practical approach employing <entity id="H01-1070.1">n-gram models</entity> and <entity id="H01-1070.2">error-correction rules</entity> for <entity id="H01-1070.3">Thai key prediction</entity> and <entity id="H01-1070.4">Thai-English language identification</entity> . The paper also proposes <entity id="H01-1070.5">rule-reduction algorithm</entity> applying <entity id="H01-1070.6">mutual information</entity> to reduce the <entity id="H01-1070.7">error-correction rules</entity> . Our algorithm reported more than 99% <entity id="H01-1070.8">accuracy</entity> in both <entity id="H01-1070.9">language identification</entity> and <entity id="H01-1070.10">key prediction</entity> . </abstract>


</text>

<text id="N01-1003"> <title>SPoT: A Trainable Sentence Planner</title> <abstract> <entity id="N01-1003.1">Sentence planning</entity> is a set of inter-related but distinct tasks, one of which is <entity id="N01-1003.2">sentence scoping</entity> , i.e. the choice of <entity id="N01-1003.3">syntactic structure</entity> for elementary <entity id="N01-1003.4">speech acts</entity> and the decision of how to combine them into one or more <entity id="N01-1003.5">sentences</entity> . In this paper, we present <entity id="N01-1003.6">SPoT</entity> , a <entity id="N01-1003.7">sentence planner</entity> , and a new methodology for automatically training <entity id="N01-1003.8">SPoT</entity> on the basis of <entity id="N01-1003.9">feedback</entity> provided by <entity id="N01-1003.10">human judges</entity> . We reconceptualize the task into two distinct phases. First, a very simple, <entity id="N01-1003.11">randomized sentence-plan-generator (SPG)</entity> generates a potentially large list of possible <entity id="N01-1003.12">sentence plans</entity> for a given <entity id="N01-1003.13">text-plan input</entity> . Second, the <entity id="N01-1003.14">sentence-plan-ranker (SPR)</entity> ranks the list of output <entity id="N01-1003.15">sentence plans</entity> , and then selects the top-ranked <entity id="N01-1003.16">plan</entity> . The <entity id="N01-1003.17">SPR</entity> uses <entity id="N01-1003.18">ranking rules</entity> automatically learned from <entity id="N01-1003.19">training data</entity> . We show that the trained <entity id="N01-1003.20">SPR</entity> learns to select a <entity id="N01-1003.21">sentence plan</entity> whose rating on average is only 5% worse than the <entity id="N01-1003.22">top human-ranked sentence plan</entity> . </abstract>


</text>

<text id="P01-1004"> <title>Low-cost, High-performance Translation Retrieval: Dumber is Better</title> <abstract> In this paper, we compare the relative effects of <entity id="P01-1004.1">segment order</entity> , <entity id="P01-1004.2">segmentation</entity> and <entity id="P01-1004.3">segment contiguity</entity> on the <entity id="P01-1004.4">retrieval performance</entity> of a <entity id="P01-1004.5">translation memory system</entity> . We take a selection of both <entity id="P01-1004.6">bag-of-words and segment order-sensitive string comparison methods</entity> , and run each over both <entity id="P01-1004.7">character- and word-segmented data</entity> , in combination with a range of <entity id="P01-1004.8">local segment contiguity models</entity> (in the form of <entity id="P01-1004.9">N-grams</entity> ). Over two distinct <entity id="P01-1004.10">datasets</entity> , we find that <entity id="P01-1004.11">indexing</entity> according to simple <entity id="P01-1004.12">character bigrams</entity> produces a <entity id="P01-1004.13">retrieval accuracy</entity> superior to any of the tested <entity id="P01-1004.14">word N-gram models</entity> . Further,in their optimum <entity id="P01-1004.15">configuration</entity> , <entity id="P01-1004.16">bag-of-words methods</entity> are shown to be equivalent to <entity id="P01-1004.17">segment order-sensitive methods</entity> in terms of <entity id="P01-1004.18">retrieval accuracy</entity> , but much faster. We also provide evidence that our findings are scalable. </abstract>


</text>

<text id="P01-1007"> <title>Guided Parsing of Range Concatenation Languages</title> <abstract> The theoretical study of the <entity id="P01-1007.1">range concatenation grammar [RCG] formalism</entity> has revealed many attractive properties which may be used in <entity id="P01-1007.2">NLP</entity> . In particular, <entity id="P01-1007.3">range concatenation languages [RCL]</entity> can be parsed in <entity id="P01-1007.4">polynomial time</entity> and many classical <entity id="P01-1007.5">grammatical formalisms</entity> can be translated into equivalent <entity id="P01-1007.6">RCGs</entity> without increasing their <entity id="P01-1007.7">worst-case parsing time complexity</entity> . For example, after <entity id="P01-1007.8">translation</entity> into an equivalent <entity id="P01-1007.9">RCG</entity> , any <entity id="P01-1007.10">tree adjoining grammar</entity> can be parsed in <entity id="P01-1007.11">O(n6) time</entity> . In this paper, we study a <entity id="P01-1007.12">parsing technique</entity> whose purpose is to improve the practical efficiency of <entity id="P01-1007.13">RCL parsers</entity> . The <entity id="P01-1007.14">non-deterministic parsing choices</entity> of the <entity id="P01-1007.15">main parser</entity> for a <entity id="P01-1007.16">language L</entity> are directed by a <entity id="P01-1007.17">guide</entity> which uses the <entity id="P01-1007.18">shared derivation forest</entity> output by a prior <entity id="P01-1007.19">RCL parser</entity> for a suitable <entity id="P01-1007.20">superset of L</entity> . The results of a practical evaluation of this method on a <entity id="P01-1007.21">wide coverage English grammar</entity> are given. </abstract>


</text>

<text id="P01-1008"> <title>Extracting Paraphrases from a Parallel Corpus</title> <abstract> While <entity id="P01-1008.1">paraphrasing</entity> is critical both for <entity id="P01-1008.2">interpretation and generation of natural language</entity> , current systems use manual or semi-automatic methods to collect <entity id="P01-1008.3">paraphrases</entity> . We present an <entity id="P01-1008.4">unsupervised learning algorithm</entity> for <entity id="P01-1008.5">identification of paraphrases</entity> from a <entity id="P01-1008.6">corpus of multiple English translations</entity> of the same <entity id="P01-1008.7">source text</entity> . Our approach yields <entity id="P01-1008.8">phrasal and single word lexical paraphrases</entity> as well as <entity id="P01-1008.9">syntactic paraphrases</entity> . </abstract>


</text>

<text id="P01-1009"> <title>Alternative Phrases and Natural Language Information Retrieval</title> <abstract> This paper presents a <entity id="P01-1009.1">formal analysis</entity> for a large class of <entity id="P01-1009.2">words</entity> called <entity id="P01-1009.3">alternative markers</entity> , which includes <entity id="P01-1009.4">other (than)</entity> , <entity id="P01-1009.5">such (as)</entity> , and <entity id="P01-1009.6">besides</entity> . These <entity id="P01-1009.7">words</entity> appear frequently enough in <entity id="P01-1009.8">dialog</entity> to warrant serious <entity id="P01-1009.9">attention</entity> , yet present <entity id="P01-1009.10">natural language search engines</entity> perform poorly on <entity id="P01-1009.11">queries</entity> containing them. I show that the <entity id="P01-1009.12">performance</entity> of a <entity id="P01-1009.13">search engine</entity> can be improved dramatically by incorporating an approximation of the <entity id="P01-1009.14">formal analysis</entity> that is compatible with the <entity id="P01-1009.15">search engine</entity> &apos;s <entity id="P01-1009.16">operational semantics</entity> . The value of this approach is that as the <entity id="P01-1009.17">operational semantics</entity> of <entity id="P01-1009.18">natural language applications</entity> improve, even larger improvements are possible. </abstract>


</text>

<text id="P01-1047"> <title>Extending Lambek grammars: a logical account of minimalist grammars</title> <abstract> We provide a <entity id="P01-1047.1">logical definition</entity> of <entity id="P01-1047.2">Minimalist grammars</entity> , that are <entity id="P01-1047.3">Stabler&apos;s formalization</entity> of <entity id="P01-1047.4">Chomsky&apos;s minimalist program</entity> . Our <entity id="P01-1047.5">logical definition</entity> leads to a neat relation to <entity id="P01-1047.6">categorial grammar</entity> , (yielding a treatment of <entity id="P01-1047.7">Montague semantics</entity> ), a <entity id="P01-1047.8">parsing-as-deduction</entity> in a <entity id="P01-1047.9">resource sensitive logic</entity> , and a <entity id="P01-1047.10">learning algorithm</entity> from <entity id="P01-1047.11">structured data</entity> (based on a <entity id="P01-1047.12">typing-algorithm</entity> and <entity id="P01-1047.13">type-unification</entity> ). Here we emphasize the connection to <entity id="P01-1047.14">Montague semantics</entity> which can be viewed as a <entity id="P01-1047.15">formal computation</entity> of the <entity id="P01-1047.16">logical form</entity> . </abstract>


</text>

<text id="P01-1056"> <title>Evaluating a Trainable Sentence Planner for a Spoken Dialogue System</title> <abstract> <entity id="P01-1056.1">Techniques for automatically training</entity> modules of a <entity id="P01-1056.2">natural language generator</entity> have recently been proposed, but a fundamental concern is whether the <entity id="P01-1056.3">quality</entity> of <entity id="P01-1056.4">utterances</entity> produced with <entity id="P01-1056.5">trainable components</entity> can compete with <entity id="P01-1056.6">hand-crafted template-based or rule-based approaches</entity> . In this paper We experimentally evaluate a <entity id="P01-1056.7">trainable sentence planner</entity> for a <entity id="P01-1056.8">spoken dialogue system</entity> by eliciting <entity id="P01-1056.9">subjective human judgments</entity> . In order to perform an exhaustive comparison, we also evaluate a <entity id="P01-1056.10">hand-crafted template-based generation component</entity> , two <entity id="P01-1056.11">rule-based sentence planners</entity> , and two <entity id="P01-1056.12">baseline sentence planners</entity> . We show that the <entity id="P01-1056.13">trainable sentence planner</entity> performs better than the <entity id="P01-1056.14">rule-based systems</entity> and the <entity id="P01-1056.15">baselines</entity> , and as well as the <entity id="P01-1056.16">hand-crafted system</entity> . </abstract>


</text>

<text id="P01-1070"> <title>Using Machine Learning Techniques to Interpret WH-questions</title> <abstract> We describe a set of <entity id="P01-1070.1">supervised machine learning</entity> experiments centering on the construction of <entity id="P01-1070.2">statistical models</entity> of <entity id="P01-1070.3">WH-questions</entity> . These <entity id="P01-1070.4">models</entity> , which are built from <entity id="P01-1070.5">shallow linguistic features</entity> of <entity id="P01-1070.6">questions</entity> , are employed to predict target variables which represent a <entity id="P01-1070.7">user&apos;s informational goals</entity> . We report on different aspects of the <entity id="P01-1070.8">predictive performance</entity> of our <entity id="P01-1070.9">models</entity> , including the influence of various <entity id="P01-1070.10">training and testing factors</entity> on <entity id="P01-1070.11">predictive performance</entity> , and examine the relationships among the target variables. </abstract>


</text>

<text id="N03-1001"> <title>Effective Utterance Classification with Unsupervised Phonotactic Models</title> <abstract> This paper describes a method for <entity id="N03-1001.1">utterance classification</entity> that does not require <entity id="N03-1001.2">manual transcription</entity> of <entity id="N03-1001.3">training data</entity> . The method combines <entity id="N03-1001.4">domain independent acoustic models</entity> with off-the-shelf <entity id="N03-1001.5">classifiers</entity> to give <entity id="N03-1001.6">utterance classification performance</entity> that is surprisingly close to what can be achieved using conventional <entity id="N03-1001.7">word-trigram recognition</entity> requiring <entity id="N03-1001.8">manual transcription</entity> . In our method, <entity id="N03-1001.9">unsupervised training</entity> is first used to train a <entity id="N03-1001.10">phone n-gram model</entity> for a particular <entity id="N03-1001.11">domain</entity> ; the <entity id="N03-1001.12">output</entity> of <entity id="N03-1001.13">recognition</entity> with this <entity id="N03-1001.14">model</entity> is then passed to a <entity id="N03-1001.15">phone-string classifier</entity> . The <entity id="N03-1001.16">classification accuracy</entity> of the method is evaluated on three different <entity id="N03-1001.17">spoken language system domains</entity> . </abstract>


</text>

<text id="N03-1004"> <title>In Question Answering, Two Heads Are Better Than One</title> <abstract> Motivated by the success of <entity id="N03-1004.1">ensemble methods</entity> in <entity id="N03-1004.2">machine learning</entity> and other areas of <entity id="N03-1004.3">natural language processing</entity> , we developed a <entity id="N03-1004.4">multi-strategy and multi-source approach to question answering</entity> which is based on combining the results from different <entity id="N03-1004.5">answering agents</entity> searching for <entity id="N03-1004.6">answers</entity> in multiple <entity id="N03-1004.7">corpora</entity> . The <entity id="N03-1004.8">answering agents</entity> adopt fundamentally different strategies, one utilizing primarily <entity id="N03-1004.9">knowledge-based mechanisms</entity> and the other adopting <entity id="N03-1004.10">statistical techniques</entity> . We present our <entity id="N03-1004.11">multi-level answer resolution algorithm</entity> that combines results from the <entity id="N03-1004.12">answering agents</entity> at the <entity id="N03-1004.13">question, passage, and/or answer levels</entity> . Experiments evaluating the effectiveness of our <entity id="N03-1004.14">answer resolution algorithm</entity> show a 35.0% relative improvement over our <entity id="N03-1004.15">baseline system</entity> in the number of <entity id="N03-1004.16">questions correctly answered</entity> , and a 32.8% improvement according to the <entity id="N03-1004.17">average precision metric</entity> . </abstract>


</text>

<text id="N03-1012"> <title>Semantic Coherence Scoring Using an Ontology</title> <abstract> In this paper we present <entity id="N03-1012.1">ONTOSCORE</entity> , a system for scoring sets of <entity id="N03-1012.2">concepts</entity> on the basis of an <entity id="N03-1012.3">ontology</entity> . We apply our system to the task of <entity id="N03-1012.4">scoring</entity> alternative <entity id="N03-1012.5">speech recognition hypotheses (SRH)</entity> in terms of their <entity id="N03-1012.6">semantic coherence</entity> . We conducted an <entity id="N03-1012.7">annotation experiment</entity> and showed that <entity id="N03-1012.8">human annotators</entity> can reliably differentiate between semantically coherent and incoherent <entity id="N03-1012.9">speech recognition hypotheses</entity> . An evaluation of our system against the <entity id="N03-1012.10">annotated data</entity> shows that, it successfully classifies 73.2% in a <entity id="N03-1012.11">German corpus</entity> of 2.284 <entity id="N03-1012.12">SRHs</entity> as either coherent or incoherent (given a <entity id="N03-1012.13">baseline</entity> of 54.55%). </abstract>


</text>

<text id="N03-1017"> <title>Statistical Phrase-Based Translation</title> <abstract> We propose a new <entity id="N03-1017.1">phrase-based translation model</entity> and <entity id="N03-1017.2">decoding algorithm</entity> that enables us to evaluate and compare several, previously proposed <entity id="N03-1017.3">phrase-based translation models</entity> . Within our framework, we carry out a large number of experiments to understand better and explain why <entity id="N03-1017.4">phrase-based models</entity> outperform <entity id="N03-1017.5">word-based models</entity> . Our empirical results, which hold for all examined <entity id="N03-1017.6">language pairs</entity> , suggest that the highest levels of performance can be obtained through relatively simple means: <entity id="N03-1017.7">heuristic learning</entity> of <entity id="N03-1017.8">phrase translations</entity> from <entity id="N03-1017.9">word-based alignments</entity> and <entity id="N03-1017.10">lexical weighting</entity> of <entity id="N03-1017.11">phrase translations</entity> . Surprisingly, learning <entity id="N03-1017.12">phrases</entity> longer than three <entity id="N03-1017.13">words</entity> and learning <entity id="N03-1017.14">phrases</entity> from <entity id="N03-1017.15">high-accuracy word-level alignment models</entity> does not have a strong impact on performance. Learning only <entity id="N03-1017.16">syntactically motivated phrases</entity> degrades the performance of our systems. </abstract>


</text>

<text id="N03-1018"> <title>A Generative Probabilistic OCR Model for NLP Applications</title> <abstract> In this paper, we introduce a <entity id="N03-1018.1">generative probabilistic optical character recognition (OCR) model</entity> that describes an end-to-end process in the <entity id="N03-1018.2">noisy channel framework</entity> , progressing from generation of <entity id="N03-1018.3">true text</entity> through its transformation into the <entity id="N03-1018.4">noisy output</entity> of an <entity id="N03-1018.5">OCR system</entity> . The <entity id="N03-1018.6">model</entity> is designed for use in <entity id="N03-1018.7">error correction</entity> , with a focus on <entity id="N03-1018.8">post-processing</entity> the <entity id="N03-1018.9">output</entity> of black-box <entity id="N03-1018.10">OCR systems</entity> in order to make it more useful for <entity id="N03-1018.11">NLP tasks</entity> . We present an implementation of the <entity id="N03-1018.12">model</entity> based on <entity id="N03-1018.13">finite-state models</entity> , demonstrate the <entity id="N03-1018.14">model</entity> &apos;s ability to significantly reduce <entity id="N03-1018.15">character and word error rate</entity> , and provide evaluation results involving <entity id="N03-1018.16">automatic extraction</entity> of <entity id="N03-1018.17">translation lexicons</entity> from <entity id="N03-1018.18">printed text</entity> . </abstract>


</text>

<text id="N03-1026"> <title>Statistical Sentence Condensation using Ambiguity Packing and Stochastic Disambiguation Methods for Lexical-Functional Grammar</title> <abstract> We present an application of <entity id="N03-1026.1">ambiguity packing and stochastic disambiguation techniques</entity> for <entity id="N03-1026.2">Lexical-Functional Grammars (LFG)</entity> to the domain of <entity id="N03-1026.3">sentence condensation</entity> . Our system incorporates a <entity id="N03-1026.4">linguistic parser/generator</entity> for <entity id="N03-1026.5">LFG</entity> , a <entity id="N03-1026.6">transfer component</entity> for <entity id="N03-1026.7">parse reduction</entity> operating on <entity id="N03-1026.8">packed parse forests</entity> , and a <entity id="N03-1026.9">maximum-entropy model</entity> for <entity id="N03-1026.10">stochastic output selection</entity> . Furthermore, we propose the use of standard <entity id="N03-1026.11">parser evaluation methods</entity> for automatically evaluating the <entity id="N03-1026.12">summarization</entity> quality of <entity id="N03-1026.13">sentence condensation systems</entity> . An <entity id="N03-1026.14">experimental evaluation</entity> of <entity id="N03-1026.15">summarization</entity> quality shows a close correlation between the <entity id="N03-1026.16">automatic parse-based evaluation</entity> and a <entity id="N03-1026.17">manual evaluation</entity> of generated <entity id="N03-1026.18">strings</entity> . Overall <entity id="N03-1026.19">summarization</entity> quality of the proposed system is state-of-the-art, with guaranteed <entity id="N03-1026.20">grammaticality</entity> of the <entity id="N03-1026.21">system output</entity> due to the use of a <entity id="N03-1026.22">constraint-based parser/generator</entity> . </abstract>


</text>

<text id="N03-1033"> <title>Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network</title> <abstract> We present a new <entity id="N03-1033.1">part-of-speech tagger</entity> that demonstrates the following ideas: (i) explicit use of both preceding and following <entity id="N03-1033.2">tag contexts</entity> via a <entity id="N03-1033.3">dependency network representation</entity> , (ii) broad use of <entity id="N03-1033.4">lexical features</entity> , including <entity id="N03-1033.5">jointly conditioning on multiple consecutive words</entity> , (iii) effective use of <entity id="N03-1033.6">priors</entity> in <entity id="N03-1033.7">conditional loglinear models</entity> , and (iv) fine-grained modeling of <entity id="N03-1033.8">unknown word features</entity> . Using these ideas together, the resulting <entity id="N03-1033.9">tagger</entity> gives a 97.24% <entity id="N03-1033.10">accuracy</entity> on the <entity id="N03-1033.11">Penn Treebank WSJ</entity> , an <entity id="N03-1033.12">error reduction</entity> of 4.4% on the best previous single automatically learned <entity id="N03-1033.13">tagging</entity> result. </abstract>


</text>

<text id="N03-2003"> <title>Getting More Mileage from Web Text Sources for Conversational Speech Language Modeling using Class-Dependent Mixtures</title> <abstract> Sources of <entity id="N03-2003.1">training data</entity> suitable for <entity id="N03-2003.2">language modeling</entity> of <entity id="N03-2003.3">conversational speech</entity> are limited. In this paper, we show how <entity id="N03-2003.4">training data</entity> can be supplemented with <entity id="N03-2003.5">text</entity> from the <entity id="N03-2003.6">web</entity> filtered to match the <entity id="N03-2003.7">style</entity> and/or <entity id="N03-2003.8">topic</entity> of the target <entity id="N03-2003.9">recognition task</entity> , but also that it is possible to get bigger performance gains from the <entity id="N03-2003.10">data</entity> by using <entity id="N03-2003.11">class-dependent interpolation</entity> of <entity id="N03-2003.12">N-grams</entity> . </abstract>


</text>

<text id="N03-2006"> <title>Adaptation Using Out-of-Domain Corpus within EBMT</title> <abstract> In order to boost the <entity id="N03-2006.1">translation quality</entity> of <entity id="N03-2006.2">EBMT</entity> based on a small-sized <entity id="N03-2006.3">bilingual corpus</entity> , we use an out-of-domain <entity id="N03-2006.4">bilingual corpus</entity> and, in addition, the <entity id="N03-2006.5">language model</entity> of an in-domain <entity id="N03-2006.6">monolingual corpus</entity> . We conducted experiments with an <entity id="N03-2006.7">EBMT system</entity> . The two <entity id="N03-2006.8">evaluation measures</entity> of the <entity id="N03-2006.9">BLEU score</entity> and the <entity id="N03-2006.10">NIST score</entity> demonstrated the effect of using an out-of-domain <entity id="N03-2006.11">bilingual corpus</entity> and the possibility of using the <entity id="N03-2006.12">language model</entity> . </abstract>


</text>

<text id="N03-2015"> <title>Unsupervised Learning of Morphology for English and Inuktitut</title> <abstract> We describe a simple <entity id="N03-2015.1">unsupervised technique</entity> for learning <entity id="N03-2015.2">morphology</entity> by identifying <entity id="N03-2015.3">hubs</entity> in an <entity id="N03-2015.4">automaton</entity> . For our purposes, a <entity id="N03-2015.5">hub</entity> is a <entity id="N03-2015.6">node</entity> in a <entity id="N03-2015.7">graph</entity> with <entity id="N03-2015.8">in-degree</entity> greater than one and <entity id="N03-2015.9">out-degree</entity> greater than one. We create a <entity id="N03-2015.10">word-trie</entity> , transform it into a <entity id="N03-2015.11">minimal DFA</entity> , then identify <entity id="N03-2015.12">hubs</entity> . Those <entity id="N03-2015.13">hubs</entity> mark the boundary between <entity id="N03-2015.14">root</entity> and <entity id="N03-2015.15">suffix</entity> , achieving similar <entity id="N03-2015.16">performance</entity> to more complex mixtures of techniques. </abstract>


</text>

<text id="N03-2017"> <title>Word Alignment with Cohesion Constraint</title> <abstract> We present a <entity id="N03-2017.1">syntax-based constraint</entity> for <entity id="N03-2017.2">word alignment</entity> , known as the <entity id="N03-2017.3">cohesion constraint</entity> . It requires disjoint <entity id="N03-2017.4">English phrases</entity> to be mapped to non-overlapping intervals in the <entity id="N03-2017.5">French sentence</entity> . We evaluate the utility of this <entity id="N03-2017.6">constraint</entity> in two different algorithms. The results show that it can provide a significant improvement in <entity id="N03-2017.7">alignment quality</entity> . </abstract>


</text>

<text id="N03-2025"> <title>Bootstrapping for Named Entity Tagging Using Concept-based Seeds</title> <abstract> A novel <entity id="N03-2025.1">bootstrapping approach</entity> to <entity id="N03-2025.2">Named Entity (NE) tagging</entity> using <entity id="N03-2025.3">concept-based seeds</entity> and <entity id="N03-2025.4">successive learners</entity> is presented. This approach only requires a few <entity id="N03-2025.5">common noun</entity> or <entity id="N03-2025.6">pronoun</entity> <entity id="N03-2025.7">seeds</entity> that correspond to the <entity id="N03-2025.8">concept</entity> for the targeted <entity id="N03-2025.9">NE</entity> , e.g. he/she/man/woman for <entity id="N03-2025.10">PERSON NE</entity> . The <entity id="N03-2025.11">bootstrapping procedure</entity> is implemented as training two <entity id="N03-2025.12">successive learners</entity> . First, <entity id="N03-2025.13">decision list</entity> is used to learn the <entity id="N03-2025.14">parsing-based NE rules</entity> . Then, a <entity id="N03-2025.15">Hidden Markov Model</entity> is trained on a <entity id="N03-2025.16">corpus</entity> automatically tagged by the first <entity id="N03-2025.17">learner</entity> . The resulting <entity id="N03-2025.18">NE system</entity> approaches <entity id="N03-2025.19">supervised NE</entity> performance for some <entity id="N03-2025.20">NE types</entity> . </abstract>


</text>

<text id="N03-2036"> <title>A Phrase-Based Unigram Model for Statistical Machine Translation</title> <abstract> In this paper, we describe a <entity id="N03-2036.1">phrase-based unigram model</entity> for <entity id="N03-2036.2">statistical machine translation</entity> that uses a much simpler set of <entity id="N03-2036.3">model parameters</entity> than similar <entity id="N03-2036.4">phrase-based models</entity> . The <entity id="N03-2036.5">units of translation</entity> are <entity id="N03-2036.6">blocks</entity> - pairs of <entity id="N03-2036.7">phrases</entity> . During <entity id="N03-2036.8">decoding</entity> , we use a <entity id="N03-2036.9">block unigram model</entity> and a <entity id="N03-2036.10">word-based trigram language model</entity> . During <entity id="N03-2036.11">training</entity> , the <entity id="N03-2036.12">blocks</entity> are learned from <entity id="N03-2036.13">source interval projections</entity> using an underlying <entity id="N03-2036.14">word alignment</entity> . We show experimental results on <entity id="N03-2036.15">block selection criteria</entity> based on <entity id="N03-2036.16">unigram</entity> counts and <entity id="N03-2036.17">phrase</entity> length. </abstract>


</text>

<text id="N03-3010"> <title>Cooperative Model Based Language Understanding in Dialogue</title> <abstract> In this paper, we propose a novel <entity id="N03-3010.1">Cooperative Model</entity> for <entity id="N03-3010.2">natural language understanding</entity> in a <entity id="N03-3010.3">dialogue system</entity> . We build this based on both <entity id="N03-3010.4">Finite State Model (FSM)</entity> and <entity id="N03-3010.5">Statistical Learning Model (SLM)</entity> . <entity id="N03-3010.6">FSM</entity> provides two strategies for <entity id="N03-3010.7">language understanding</entity> and have a high accuracy but little robustness and flexibility. <entity id="N03-3010.8">Statistical approach</entity> is much more robust but less accurate. <entity id="N03-3010.9">Cooperative Model</entity> incorporates all the three strategies together and thus can suppress all the shortcomings of different strategies and has all the advantages of the three strategies. </abstract>


</text>

<text id="N03-4010"> <title>JAVELIN: A Flexible, Planner-Based Architecture for Question Answering</title> <abstract> The <entity id="N03-4010.1">JAVELIN system</entity> integrates a flexible, <entity id="N03-4010.2">planning-based architecture</entity> with a variety of <entity id="N03-4010.3">language processing modules</entity> to provide an <entity id="N03-4010.4">open-domain question answering capability</entity> on <entity id="N03-4010.5">free text</entity> . The demonstration will focus on how <entity id="N03-4010.6">JAVELIN</entity> processes <entity id="N03-4010.7">questions</entity> and retrieves the most likely <entity id="N03-4010.8">answer candidates</entity> from the given <entity id="N03-4010.9">text corpus</entity> . The operation of the system will be explained in depth through browsing the <entity id="N03-4010.10">repository</entity> of <entity id="N03-4010.11">data objects</entity> created by the system during each <entity id="N03-4010.12">question answering session</entity> . </abstract>


</text>


</doc>
