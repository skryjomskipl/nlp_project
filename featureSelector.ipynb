{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset from path: data/subtask11/1.1.train.text.xml\n",
      "Reading relations from path: data/subtask11/1.1.train.relations.txt\n",
      "Reading dataset from path: data/subtask11/1.1.test.text.xml\n",
      "Reading relations from path: data/subtask11/1.1.test.relations.txt\n",
      "Reading dataset from path: data/subtask11/1.1.test.text.xml\n",
      "Reading relations from path: data/subtask11/1.1.test.key.txt\n"
     ]
    }
   ],
   "source": [
    "# This file created by chathuri to find out best feature combination\n",
    "\n",
    "from dataset import *\n",
    "from common import *\n",
    "from modules import *\n",
    "import itertools\n",
    "import code\n",
    "import copy\n",
    "import random\n",
    "import nltk\n",
    "#nltk.download()\n",
    "utils = Utils()\n",
    "train = Dataset('data/subtask11/1.1.train.text.xml', 'data/subtask11/1.1.train.relations.txt')\n",
    "train.read(utils)\n",
    "\n",
    "test = Dataset('data/subtask11/1.1.test.text.xml', 'data/subtask11/1.1.test.relations.txt', test_dataset = True)\n",
    "test.read(utils)\n",
    "\n",
    "test_key = Dataset('data/subtask11/1.1.test.text.xml', 'data/subtask11/1.1.test.key.txt')\n",
    "test_key.read(utils)\n",
    "\n",
    "# Prepare utilities\n",
    "utils = Utils()\n",
    "\n",
    "featuresRoot=FeatureRoot(utils)\n",
    "tfIdfCalculator=TfIdfCalculator(utils);\n",
    "\n",
    "# Enable features in this order: Przemek, Samantha, Chathuri\n",
    "features_state = [False,False ,  True]\n",
    "\n",
    "\n",
    "# Create training set\n",
    "featuresRoot.set_dataset(train)\n",
    "\n",
    "#calculate tf idf for the whole training data set abstracts\n",
    "tfIdfCalculator.set_dataset(train)\n",
    "s=tfIdfCalculator.calc_ifidf_data()\n",
    "\n",
    "\n",
    "##train_X, train_Y = features.prepare_data(features_state)\n",
    "train_X1, train_Y1 = featuresRoot.prepare_data(features_state)\n",
    "\n",
    "# Create test set\n",
    "featuresRoot.set_dataset(test)\n",
    "\n",
    "tfIdfCalculator.set_dataset(test)\n",
    "s=tfIdfCalculator.calc_ifidf_data()\n",
    "\n",
    "test_X1, test_Y1 = featuresRoot.prepare_data(features_state, test_dataset = True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print((train_X1))\n",
    "stuff=[1,2]#,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18];\n",
    "setsTrain=[]\n",
    "setsFea=[]\n",
    "for L in range(1, len(train_X1[1])+1):# define array of features and get len of it \n",
    "    for subset in itertools.combinations(stuff, L):\n",
    "        #print(\"sub>>>>>>>>> \")\n",
    "        #print(subset)\n",
    "        setsFea.append(subset)\n",
    "        set2=[]\n",
    "        for i in range(len(train_X1)):#for each rel\n",
    "            set=[]\n",
    "            for j in range(len(subset)):# for each instance subset of features\n",
    "                ind=subset[j]\n",
    "                #print(ind)\n",
    "                set.append(train_X1[i][ind-1])\n",
    "            set2.append(set)\n",
    "        setsTrain.append(set2)\n",
    "        #print(set2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print((test_X1))\n",
    "\n",
    "setsTest=[]\n",
    "for L in range(1, len(test_X1[1])+1):# define array of features and get len of it \n",
    "    for subset in itertools.combinations(stuff, L):\n",
    "        #print(\"sub>>>>>>>>> \")\n",
    "        #print(subset)\n",
    "        set2=[]\n",
    "        for i in range(len(test_X1)):#for each rel\n",
    "            set=[]\n",
    "            for j in range(len(subset)):# for each instance subset of features\n",
    "                ind=subset[j]\n",
    "                #print(ind)\n",
    "                set.append(test_X1[i][ind-1])\n",
    "            set2.append(set)\n",
    "        setsTest.append(set2)\n",
    "        #print(set2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print((sets[1]))\n",
    "#print(len(setsTrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy1: 48.0%\n",
      "F-Measure1: 26.21%\n",
      "Accuracy1: 44.0%\n",
      "F-Measure1: 17.48%\n",
      "Accuracy1: 40.0%\n",
      "F-Measure1: 11.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brahmanacsw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "out = open('chathuAll2.csv', 'w')\n",
    "for i in range(len(setsTrain)):\n",
    "    #print(\"newset>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "    #print(\"features :\",setsFea[i-1])\n",
    "    train_x1=setsTrain[i-1]\n",
    "    test_x1=setsTest[i-1]\n",
    "    #clf = utils.get_classifier(\"Naive Bayes\")#Decision Tree, Naive Bayes, SVM\n",
    "    #clf = utils.get_classifier(\"Decision Tree\")\n",
    "    clf = utils.get_classifier(\"SVM\")\n",
    "\n",
    "    #fit dataset\n",
    "\n",
    "    clf = clf.fit(train_x1, train_Y1)\n",
    "    #take subset out \n",
    "    test_Y = clf.predict(test_x1)\n",
    "\n",
    "    #print(\"\\n=> Training set\")\n",
    "    #print(\" -> X\")\n",
    "    #print(train_X1)\n",
    "    #print(\" -> Y\")\n",
    "    #print(train_Y1)\n",
    "    #print(\"\\n=> Testing set\")\n",
    "    #print(\" -> X\")\n",
    "    #print(test_X1)\n",
    "    #print(\" -> Y\")\n",
    "    #print(test_Y1)\n",
    "\n",
    "    # Preparing results\n",
    "    #print(\"\\n=> Results\")\n",
    "    featuresRoot.set_dataset(test)\n",
    "    #featuresRoot.print_results(test_Y)\n",
    "\n",
    "    # Compute accuracy\n",
    "    featuresRoot.set_dataset(test_key)\n",
    "    #featuresRoot.set_dataset(test_Y)\n",
    "    test_key_Y = featuresRoot.get_dataset_key()\n",
    "\n",
    "\n",
    "\n",
    "    #print(\"\\n=> Metrics\")\n",
    "    print(\"Accuracy1: \", round(utils.get_accuracy(test_key_Y, test_Y), 2), \"%\", sep = '')\n",
    "    print(\"F-Measure1: \", round(utils.get_fmeasure(test_key_Y, test_Y), 2), \"%\", sep = '')\n",
    "    #print(\"Precision:          \", round(utils.get_precision(test_key_Y, test_Y), 2), \"%\", sep = '')\n",
    "    #print(\"Recall:         \", round(utils.get_recall(test_key_Y, test_Y), 2), \"%\", sep = '')\n",
    "    mystring = \"\"\n",
    "    mystring = '+'.join([str(x) for x in setsFea[i-1]])\n",
    "    #print(mystring)\n",
    "    out.write('%d;' % round(utils.get_accuracy(test_key_Y, test_Y), 2))\n",
    "    out.write('%d;' % round(utils.get_fmeasure(test_key_Y, test_Y), 2))\n",
    "    #out.write('%d;' % round(utils.get_precision(test_key_Y, test_Y), 2))\n",
    "    #out.write('%d;' % round(utils.get_recall(test_key_Y, test_Y), 2))\n",
    "    #out.write('%s;' % mystring)\n",
    "   \n",
    "    out.write('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "(4,)\n",
      "(2,)\n",
      "(2,)\n",
      "(3,)\n",
      "(3,)\n",
      "([1, 2],)\n",
      "([1, 2],)\n",
      "(4, 2)\n",
      "(4, 2)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "(4, [1, 2])\n",
      "(4, [1, 2])\n",
      "(2, 3)\n",
      "(2, 3)\n",
      "(2, [1, 2])\n",
      "(2, [1, 2])\n",
      "(3, [1, 2])\n",
      "(3, [1, 2])\n",
      "(4, 2, 3)\n",
      "(4, 2, 3)\n",
      "(4, 2, [1, 2])\n",
      "(4, 2, [1, 2])\n",
      "(4, 3, [1, 2])\n",
      "(4, 3, [1, 2])\n",
      "(2, 3, [1, 2])\n",
      "(2, 3, [1, 2])\n",
      "(4, 2, 3, [1, 2])\n",
      "(4, 2, 3, [1, 2])\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "stuff = [4, 2, 3,[1,2]]\n",
    "for L in range(1, len(stuff)+1):\n",
    "    for subset in itertools.combinations(stuff, L):\n",
    "        print(subset)\n",
    "        subset1=subset\n",
    "        print(subset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mystring = \"\"\n",
    "mystring = ','.join([str(x) for x in stuff])\n",
    "print(mystring)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
